{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banking Fraud Detection with XGBoost\n",
    "\n",
    "Detecting fraud in banking is very important for real banks. Very few of these are fraud, but if the transaction isnâ€™t well detected, the damage is huge. In this task, you will create a binary classifier for mobile money transaction data. You can also download data from [kaggle](https://www.kaggle.com/ntnu-testimon/paysim1). Since the dataset has already been processed once, the number of columns or rows may be different.\n",
    "\n",
    "## Data description\n",
    "\n",
    "This simulation dataset helps financial research, and it contains 9 columns and 300 records. Each record means one transaction that includes cash in, cash out, debit, credit, or transfer. Also each transcation has amount, name of origin and etc. The label or target column is ``isFraud`` that shows whether fraud or not.\n",
    "\n",
    "- type - CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.\n",
    "- amount - amount of the transaction in local currency.\n",
    "- nameOrig - customer who started the transaction\n",
    "- oldbalanceOrg - initial balance before the transaction\n",
    "- newbalanceOrig - new balance after the transaction\n",
    "- nameDest - customer who is the recipient of the transaction\n",
    "- oldbalanceDest - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).\n",
    "- newbalanceDest - new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants).\n",
    "- isFraud - This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.\n",
    "\n",
    "## IMPORTANT\n",
    "This notebook assumes that you have already performed data preprocessing with SageMaker Data Wrangler. Please look up this [github](https://github.com/jjk-dev/amazon-sagemaker-studio-workshop.git)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Download several libraries to proceed with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                         # For manipulating filepath names  \n",
    "import sys                                        # For writing outputs to notebook\n",
    "import math                                       # For ceiling function\n",
    "import json                                       # For parsing hosting outputs\n",
    "\n",
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd                               # For munging tabular data\n",
    "import matplotlib.pyplot as plt                   # For charts and visualizations\n",
    "from IPython.display import Image                 # For displaying images in the notebook\n",
    "from IPython.display import display               # For displaying outputs in the notebook\n",
    "from time import gmtime, strftime                 # For labeling SageMaker models, endpoints, etc.\n",
    "\n",
    "import sagemaker                                  # Amazon SageMaker's Python SDK provides many helper functions\n",
    "from sagemaker import get_execution_role          # Define IAM role\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set boto3 and variables\n",
    "\n",
    "Connect the session and search for IAM (Identity and Access Management) role. And load data then set some values such as ``S3 bucket name`` and ``student number``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name AmazonSageMaker-ExecutionRole-20210426T215985 to get Role path.\n",
      "Assuming role was created in SageMaker AWS console, as the name contains `AmazonSageMaker-ExecutionRole`. Defaulting to Role ARN with service-role in path. If this Role ARN is incorrect, please add IAM read permissions to your role or supply the Role Arn directly.\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "s3 = boto3.resource('s3')\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the values below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_number = 'CHANGE TO YOUR STUDENT NUMBER'       # e.g. '2021000000'\n",
    "bucket = 'CHANGE TO YOUR S3 BUCKET NAME'               # e.g. sagemaker-000000000000\n",
    "\n",
    "input_data_bucket = 'CHANGE TO YOUR INPUT DATA LOCATION IN S3 BUCKET'     # e.g. sagemaker-000000000000/.../default\n",
    "file = 'CHANGE TO YOUR TRANSFORMED DATA'                # e.g. part-00000-edb8e4ca....csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'banking-fraud'                           # Set folder name in S3 bucket        \n",
    "\n",
    "data_location = 's3://{}/{}'.format(input_data_bucket, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 11)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_location)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>type_PAYMENT</th>\n",
       "      <th>type_CASH_OUT</th>\n",
       "      <th>type_CASH_IN</th>\n",
       "      <th>type_TRANSFER</th>\n",
       "      <th>type_DEBIT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140421.18</td>\n",
       "      <td>16004</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>140421.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216666.53</td>\n",
       "      <td>50398</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10119297.16</td>\n",
       "      <td>10335963.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>234636.20</td>\n",
       "      <td>74262</td>\n",
       "      <td>0.00</td>\n",
       "      <td>166046.48</td>\n",
       "      <td>400682.68</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52816.29</td>\n",
       "      <td>117751</td>\n",
       "      <td>170567.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63871.25</td>\n",
       "      <td>6012</td>\n",
       "      <td>0.00</td>\n",
       "      <td>456488.36</td>\n",
       "      <td>520359.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  newbalanceDest  \\\n",
       "0  140421.18          16004            0.00            0.00       140421.18   \n",
       "1  216666.53          50398            0.00     10119297.16     10335963.70   \n",
       "2  234636.20          74262            0.00       166046.48       400682.68   \n",
       "3   52816.29         117751       170567.29            0.00            0.00   \n",
       "4   63871.25           6012            0.00       456488.36       520359.60   \n",
       "\n",
       "   isFraud  type_PAYMENT  type_CASH_OUT  type_CASH_IN  type_TRANSFER  \\\n",
       "0        0           0.0            1.0           0.0            0.0   \n",
       "1        0           0.0            1.0           0.0            0.0   \n",
       "2        0           0.0            1.0           0.0            0.0   \n",
       "3        0           0.0            0.0           1.0            0.0   \n",
       "4        0           0.0            1.0           0.0            0.0   \n",
       "\n",
       "   type_DEBIT  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete target column that unnecessary in triainin\n",
    "y_column = 'isFraud'\n",
    "columns_to_drop = [y_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data as train, validation,and test in 7:2:1\n",
    "train_data, validation_data, test_data = np.split(df.sample(frac=1, random_state=2021), [int(0.7 * len(df)), int(0.9 * len(df))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Amazon SageMaker, XGBoost container use data in libSVM or CSV format. In this lab, you use CSV file. The first column in the CSV file must be specified as the target value, and the header must not be included. You will work after splittin into train | validation | test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each dataset in local environment\n",
    "pd.concat([train_data[y_column], train_data.drop(columns_to_drop, axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "pd.concat([validation_data[y_column], validation_data.drop(columns_to_drop, axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)\n",
    "pd.concat([test_data[y_column], test_data.drop(columns_to_drop, axis=1)], axis=1).to_csv('test.csv', index=False, header=False)\n",
    "pd.concat([test_data.drop(columns_to_drop, axis=1)], axis=1).to_csv('test_features.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the file to S3 so you can acesss in managed enrionment of SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test/test_features.csv')).upload_file('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model using XGBoost\n",
    "\n",
    "In this notebook, use XGBoost which is simple but effective for binary classification. XGBoost is a open source library that conduct Gradient Boosting. This performs excellent calculation skill, implements all the necessary functions, and has been successful in many machine learning competitions. Let's start with a simple xgboost model to learn using managed, distributed learning framework.\n",
    "\n",
    "From ECR container, you can use built-in algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(framework = 'xgboost', \n",
    "                                          region = boto3.Session().region_name, \n",
    "                                          version = 'latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `s3_input` object that informs file location and set content type as csv because now you use CSV file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate estimator as specifing parameter below.\n",
    "\n",
    "- Xgboost algorithm container\n",
    "- IAM role\n",
    "- Training instance type and count (By using 'local_cpu', you can train model with in this notebook instance.)\n",
    "- Output location in S3\n",
    "- Algorithm hyperparameter\n",
    "\n",
    "Execute `.fit()` using follow value.\n",
    "- Location of train / validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-26 18:53:18 Starting - Starting the training job...\n",
      "2021-04-26 18:53:26 Starting - Launching requested ML instancesProfilerReport-1619463197: InProgress\n",
      ".........\n",
      "2021-04-26 18:55:06 Starting - Preparing the instances for training............\n",
      "2021-04-26 18:57:19 Downloading - Downloading input data\n",
      "2021-04-26 18:57:19 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2021-04-26:18:57:20:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2021-04-26:18:57:20:INFO] File size need to be processed in the node: 0.15mb. Available memory size in the node: 8405.34mb\u001b[0m\n",
      "\u001b[34m[2021-04-26:18:57:20:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[18:57:20] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[18:57:20] 2100x10 matrix with 21000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2021-04-26:18:57:20:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[18:57:20] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[18:57:20] 600x10 matrix with 6000 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[18:57:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.030476#011validation-error:0.028333\u001b[0m\n",
      "\u001b[34m[18:57:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.021905#011validation-error:0.026667\u001b[0m\n",
      "\u001b[34m[18:57:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.021905#011validation-error:0.026667\u001b[0m\n",
      "\u001b[34m[18:57:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.019048#011validation-error:0.02\u001b[0m\n",
      "\u001b[34m[18:57:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.016667#011validation-error:0.021667\u001b[0m\n",
      "\n",
      "2021-04-26 18:57:47 Uploading - Uploading generated training model\n",
      "2021-04-26 18:57:47 Completed - Training job completed\n",
      "Training seconds: 40\n",
      "Billable seconds: 40\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "job_name=student_number+'-banking-fraud-'+strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge', \n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess,\n",
    "                                    base_job_name=job_name)\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=5)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting\n",
    "### Create endpoint\n",
    "\n",
    "When the xgboost model trained on the input data, it deployed as an endpoint for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!CPU times: user 199 ms, sys: 18 ms, total: 217 ms\n",
      "Wall time: 6min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "endpoint_name=student_number+'-banking-fraud-'+strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "xgb_predictor = xgb.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge', \n",
    "                           endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference\n",
    "### Make predictions using the endpoint\n",
    "\n",
    "Compare actual and predicted values to verify the performance of \\machine learning model. Transfer the data for inference to endpoint and get the result. Serialize data into CSV format to send it as an HTTP POST request and decode CSV result.\n",
    "\n",
    "CAUTION: SageMaker XGBoost doesn't contain target column when inference as CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "xgb_predictor.serializer = CSVSerializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function that call endpoint.\n",
    "\n",
    "- Repeat test dataset (Loop)\n",
    "- Divide minibatch as number of rows\n",
    "- Transform minibatch to CSV string payloads (Delete target column)\n",
    "- Call XGBoost endpoint and send predict value\n",
    "- Transform retured CSV result to NumPy arrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 1 completed\n"
     ]
    }
   ],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for i, array in enumerate(split_array):\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "        if i % 10 == 0:\n",
    "            print(i, 'out of', len(split_array), 'completed')\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(test_data.drop(columns_to_drop, axis=1).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       265\n",
      "           1       0.90      0.80      0.85        35\n",
      "\n",
      "    accuracy                           0.97       300\n",
      "   macro avg       0.94      0.89      0.91       300\n",
      "weighted avg       0.97      0.97      0.97       300\n",
      "\n",
      "Test accuracy: 0.9666666666666667\n",
      "ROC_AUC score: 0.8943396226415095\n"
     ]
    }
   ],
   "source": [
    "# F1-score, accurancy, ROC\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score # import classification metrics\n",
    "\n",
    "print(classification_report(test_data[y_column], np.round(predictions)))\n",
    "print(\"Test accuracy:\", accuracy_score(test_data[y_column], np.round(predictions)))\n",
    "print(\"ROC_AUC score:\", roc_auc_score(test_data[y_column], np.round(predictions) / 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create confustion matrix that compares predicted result and actual value. The result may not exactly same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>262</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions  0.0  1.0\n",
       "actuals              \n",
       "0            262    3\n",
       "1              7   28"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index=test_data['isFraud'], columns=np.round(predictions), rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP, TN, FP, FN are defined as below.\n",
    "\n",
    "- TP = Truly (identified as) Positive\n",
    "- TN = Truly (identified as) Negative\n",
    "- FP = Falsely (identified as) Positive\n",
    "- FN = Falsely (identified as) Negative\n",
    "\n",
    "The confusion matrix means like this:\n",
    "\n",
    "| actuals\\predictions | 0 | 1 |\n",
    "| --- | --- | --- |\n",
    "| 0 | TN | FP |\n",
    "| 1 | FN | TP |\n",
    "\n",
    "\n",
    "On that basis, you can calculate Accuracy, Precision, Recall.\n",
    "- Accuracy = (TP + TN) / (TP + FP + FN + TP)\n",
    "- Precision = TP / (TP + FP) = 670 / (670 + 29)\n",
    "- Recall = TP / (TP + FN) = 670 / (670 + 157)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop / Close the Endpoint\n",
    "\n",
    "After finishing all of these examples, run the cell below. The following command removes the endpoint hosted on the SageMaker created in the inference step. If the endpoint exist, the charges will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-2:806072073708:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
